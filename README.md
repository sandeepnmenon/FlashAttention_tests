# FlashAttention_tests
Benchmarking different FlashAttention and FlashAttention-v2 implementations

# Flash-Attention Repo links
Standard Attention: [https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/modules/mha.py](https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/modules/mha.py#L175)
Flash Self Attention: [https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/modules/mha.py](https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/modules/mha.py#L36)
[Kernel code](https://github.com/Dao-AILab/flash-attention/blob/2c3baba4a63c4007c8a132c5380edc9430f88a22/flash_attn/flash_attn_triton.py#L66C5-L66C16)
