{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "T0KEIvTL_kaF",
        "outputId": "033c9eda-4db3-4c4d-c563-4e875a06bb3b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.1.0+cu118'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUZW57xDBN-T",
        "outputId": "bc0a1c4b-abc8-462f-8d19-a5958e533ae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Standard attention took 0.9052870273590088 seconds for 10 trials\n",
            "Flash attention took 0.31307268142700195 seconds for 10 trials\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set up the basic parameters for the model\n",
        "batch_size = 32\n",
        "sequence_length = 2048\n",
        "dimensions = 64\n",
        "number_of_heads = 8\n",
        "\n",
        "# Creating query (q), key (k), and value (v) tensors\n",
        "# These tensors are initialized with random values and moved to the GPU for faster processing\n",
        "q = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "k = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "v = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "\n",
        "# Define the dropout rate and number of trials for benchmarking\n",
        "dropout_rate = 0.2\n",
        "num_trials = 10\n",
        "\n",
        "# Standard Attention Computation\n",
        "torch.cuda.synchronize()  # Synchronizes CPU and GPU to ensure accurate timing\n",
        "start = time.time()  # Start timer\n",
        "for i in range(num_trials):\n",
        "    attn = q @ k.transpose(-2, -1)  # Compute attention scores\n",
        "    attn = attn.softmax(dim=-1)  # Apply softmax to get probabilities\n",
        "    attn = F.dropout(attn, p=dropout_rate, training=True)  # Apply dropout\n",
        "    x = (attn @ v).transpose(1, 2)  # Apply attention to value and reshape\n",
        "torch.cuda.synchronize()  # Ensure all GPU tasks are finished\n",
        "end = time.time()  # End timer\n",
        "print('Standard attention took {} seconds for {} trials'.format(end - start, num_trials))\n",
        "\n",
        "# Flash Attention Computation\n",
        "with torch.backends.cuda.sdp_kernel(\n",
        "    enable_flash=True, enable_math=False, enable_mem_efficient=False\n",
        "):\n",
        "    torch.cuda.synchronize()  # Synchronizes CPU and GPU for accurate timing\n",
        "    start = time.time()  # Start timer\n",
        "    for i in range(num_trials):\n",
        "        out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_rate)  # Compute attention using FlashAttention\n",
        "    torch.cuda.synchronize()  # Ensure completion of all GPU tasks\n",
        "    end = time.time()  # End timer\n",
        "    print('Flash attention took {} seconds for {} trials'.format(end - start, num_trials))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWat3ADACDxg",
        "outputId": "832f5cc6-05e3-46ed-9c1e-d7c4bf8cc42d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.3.3.tar.gz (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.1.0+cu118)\n",
            "Collecting einops (from flash-attn)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (23.2)\n",
            "Collecting ninja (from flash-attn)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.3.3-cp310-cp310-linux_x86_64.whl size=57075008 sha256=bcb63b64213ab61590b340b77de84e448a442e19c100480895194df39ad7673d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/e6/fa/941802ec61d1afd320d27160ab1db98e6dba65381f84b76d4a\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: ninja, einops, flash-attn\n",
            "Successfully installed einops-0.7.0 flash-attn-2.3.3 ninja-1.11.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVPZE-n6Y6vA"
      },
      "outputs": [],
      "source": [
        "from flash_attn import flash_attn_qkvpacked_func, flash_attn_func\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfiQgwzLZDRE",
        "outputId": "53038981-4126-4db9-8c4e-b333399d3c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'flash-attention'...\n",
            "remote: Enumerating objects: 4303, done.\u001b[K\n",
            "remote: Counting objects: 100% (1785/1785), done.\u001b[K\n",
            "remote: Compressing objects: 100% (191/191), done.\u001b[K\n",
            "remote: Total 4303 (delta 1632), reused 1606 (delta 1590), pack-reused 2518\u001b[K\n",
            "Receiving objects: 100% (4303/4303), 6.88 MiB | 17.61 MiB/s, done.\n",
            "Resolving deltas: 100% (3007/3007), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Dao-AILab/flash-attention.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aaz0U3PVZSjU",
        "outputId": "91a9633a-f990-4668-de4a-dd3de0b3c747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/flash-attention\n"
          ]
        }
      ],
      "source": [
        "%cd flash-attention/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcNc8nCAZXvh",
        "outputId": "f1fdbe4f-6a74-4b5b-acf4-ca941a8a6cd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-111-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 111, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  6.5869e-01,\n",
            "           -3.5547e-01, -1.9111e+00],\n",
            "          [... -1.2842e+00,\n",
            "           -1.8906e+00, -1.4766e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 2.2695e+00, -3.8940e-01,  1.2666e+00,  ..., -5.9424e-01,\n",
            "           -3.2031e-01,  2.0176e+00],\n",
            "          [... -5.6348e-01,\n",
            "            2.6484e+00,  2.9907e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.0186e+00, -4.6436e-01,  1.2949e+00,  ..., -8.7341e-02,\n",
            "            3.5205e-01, -1.3994e+00],\n",
            "          [... -8.1104e-01,\n",
            "            6.2842e-01, -1.6223e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.0949157995752499, causal = True, window_size = tensor([44, 39])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________ test_flash_attn_qkvpacked[0.0-200-128-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 128, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -2.1277e-01,\n",
            "           -3.3154e-01, -2.0227e-01],\n",
            "          [... -1.2079e-01,\n",
            "           -1.1904e+00, -5.0537e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 7.3975e-01, -8.1592e-01,  2.2852e-01,  ..., -5.4248e-01,\n",
            "            3.7903e-02,  6.7725e-01],\n",
            "          [... -4.8035e-02,\n",
            "            1.4941e-01,  1.1963e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 7.0850e-01, -5.7471e-01,  8.4229e-01,  ..., -2.6660e-01,\n",
            "           -1.7461e+00,  1.4756e+00],\n",
            "          [... -3.4155e-01,\n",
            "            3.0835e-01,  6.8457e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.08838834764831845, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-128-False-True-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 128, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -2.1277e-01,\n",
            "           -3.3154e-01, -2.0227e-01],\n",
            "          [... -1.2079e-01,\n",
            "           -1.1904e+00, -5.0537e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 7.3975e-01, -8.1592e-01,  2.2852e-01,  ..., -5.4248e-01,\n",
            "            3.7903e-02,  6.7725e-01],\n",
            "          [... -4.8035e-02,\n",
            "            1.4941e-01,  1.1963e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 7.0850e-01, -5.7471e-01,  8.4229e-01,  ..., -2.6660e-01,\n",
            "           -1.7461e+00,  1.4756e+00],\n",
            "          [... -3.4155e-01,\n",
            "            3.0835e-01,  6.8457e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.08838834764831845, causal = False, window_size = tensor([44, 39])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-128-True-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 128, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -2.1277e-01,\n",
            "           -3.3154e-01, -2.0227e-01],\n",
            "          [... -1.2079e-01,\n",
            "           -1.1904e+00, -5.0537e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 7.3975e-01, -8.1592e-01,  2.2852e-01,  ..., -5.4248e-01,\n",
            "            3.7903e-02,  6.7725e-01],\n",
            "          [... -4.8035e-02,\n",
            "            1.4941e-01,  1.1963e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 7.0850e-01, -5.7471e-01,  8.4229e-01,  ..., -2.6660e-01,\n",
            "           -1.7461e+00,  1.4756e+00],\n",
            "          [... -3.4155e-01,\n",
            "            3.0835e-01,  6.8457e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.08838834764831845, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-128-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 128, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -2.1277e-01,\n",
            "           -3.3154e-01, -2.0227e-01],\n",
            "          [... -1.2079e-01,\n",
            "           -1.1904e+00, -5.0537e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 7.3975e-01, -8.1592e-01,  2.2852e-01,  ..., -5.4248e-01,\n",
            "            3.7903e-02,  6.7725e-01],\n",
            "          [... -4.8035e-02,\n",
            "            1.4941e-01,  1.1963e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 7.0850e-01, -5.7471e-01,  8.4229e-01,  ..., -2.6660e-01,\n",
            "           -1.7461e+00,  1.4756e+00],\n",
            "          [... -3.4155e-01,\n",
            "            3.0835e-01,  6.8457e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.08838834764831845, causal = True, window_size = tensor([44, 39])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________ test_flash_attn_qkvpacked[0.0-200-160-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 160, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.2852e+00,\n",
            "            7.5732e-01, -8.3154e-01],\n",
            "          [...  1.6133e+00,\n",
            "            6.9824e-01, -1.1641e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-1.4443e+00,  2.9565e-01, -2.4976e-01,  ...,  5.6299e-01,\n",
            "            2.9443e-01, -1.0242e-01],\n",
            "          [... -1.7310e-01,\n",
            "           -1.1006e+00,  1.8835e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.2002e+00,  1.6396e+00, -4.3915e-02,  ..., -7.9785e-01,\n",
            "           -2.2363e-01, -4.7534e-01],\n",
            "          [...  1.5244e+00,\n",
            "            1.2998e+00, -1.2734e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07905694150420949, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-160-False-True-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 160, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.2852e+00,\n",
            "            7.5732e-01, -8.3154e-01],\n",
            "          [...  1.6133e+00,\n",
            "            6.9824e-01, -1.1641e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-1.4443e+00,  2.9565e-01, -2.4976e-01,  ...,  5.6299e-01,\n",
            "            2.9443e-01, -1.0242e-01],\n",
            "          [... -1.7310e-01,\n",
            "           -1.1006e+00,  1.8835e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.2002e+00,  1.6396e+00, -4.3915e-02,  ..., -7.9785e-01,\n",
            "           -2.2363e-01, -4.7534e-01],\n",
            "          [...  1.5244e+00,\n",
            "            1.2998e+00, -1.2734e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07905694150420949, causal = False, window_size = tensor([44, 39])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-160-True-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 160, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.2852e+00,\n",
            "            7.5732e-01, -8.3154e-01],\n",
            "          [...  1.6133e+00,\n",
            "            6.9824e-01, -1.1641e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-1.4443e+00,  2.9565e-01, -2.4976e-01,  ...,  5.6299e-01,\n",
            "            2.9443e-01, -1.0242e-01],\n",
            "          [... -1.7310e-01,\n",
            "           -1.1006e+00,  1.8835e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.2002e+00,  1.6396e+00, -4.3915e-02,  ..., -7.9785e-01,\n",
            "           -2.2363e-01, -4.7534e-01],\n",
            "          [...  1.5244e+00,\n",
            "            1.2998e+00, -1.2734e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07905694150420949, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-160-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 160, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.2852e+00,\n",
            "            7.5732e-01, -8.3154e-01],\n",
            "          [...  1.6133e+00,\n",
            "            6.9824e-01, -1.1641e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-1.4443e+00,  2.9565e-01, -2.4976e-01,  ...,  5.6299e-01,\n",
            "            2.9443e-01, -1.0242e-01],\n",
            "          [... -1.7310e-01,\n",
            "           -1.1006e+00,  1.8835e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.2002e+00,  1.6396e+00, -4.3915e-02,  ..., -7.9785e-01,\n",
            "           -2.2363e-01, -4.7534e-01],\n",
            "          [...  1.5244e+00,\n",
            "            1.2998e+00, -1.2734e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07905694150420949, causal = True, window_size = tensor([44, 39])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________ test_flash_attn_qkvpacked[0.0-200-192-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 192, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  2.1448e-01,\n",
            "           -7.3682e-01, -4.5166e-01],\n",
            "          [...  5.2295e-01,\n",
            "            1.1475e+00, -1.1123e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 5.5420e-01, -8.8330e-01,  2.6636e-01,  ..., -5.5273e-01,\n",
            "            6.2451e-01,  1.0371e+00],\n",
            "          [... -6.7578e-01,\n",
            "           -6.5088e-01, -6.9043e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-2.7612e-01, -1.2031e+00,  7.4658e-01,  ..., -1.2822e+00,\n",
            "           -5.6836e-01, -1.7615e-01],\n",
            "          [... -8.8232e-01,\n",
            "            3.9453e-01,  1.1338e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07216878364870322, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-192-False-True-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 192, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  2.1448e-01,\n",
            "           -7.3682e-01, -4.5166e-01],\n",
            "          [...  5.2295e-01,\n",
            "            1.1475e+00, -1.1123e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 5.5420e-01, -8.8330e-01,  2.6636e-01,  ..., -5.5273e-01,\n",
            "            6.2451e-01,  1.0371e+00],\n",
            "          [... -6.7578e-01,\n",
            "           -6.5088e-01, -6.9043e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-2.7612e-01, -1.2031e+00,  7.4658e-01,  ..., -1.2822e+00,\n",
            "           -5.6836e-01, -1.7615e-01],\n",
            "          [... -8.8232e-01,\n",
            "            3.9453e-01,  1.1338e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07216878364870322, causal = False, window_size = tensor([44, 39])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-192-True-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 192, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  2.1448e-01,\n",
            "           -7.3682e-01, -4.5166e-01],\n",
            "          [...  5.2295e-01,\n",
            "            1.1475e+00, -1.1123e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 5.5420e-01, -8.8330e-01,  2.6636e-01,  ..., -5.5273e-01,\n",
            "            6.2451e-01,  1.0371e+00],\n",
            "          [... -6.7578e-01,\n",
            "           -6.5088e-01, -6.9043e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-2.7612e-01, -1.2031e+00,  7.4658e-01,  ..., -1.2822e+00,\n",
            "           -5.6836e-01, -1.7615e-01],\n",
            "          [... -8.8232e-01,\n",
            "            3.9453e-01,  1.1338e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07216878364870322, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-192-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 192, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  2.1448e-01,\n",
            "           -7.3682e-01, -4.5166e-01],\n",
            "          [...  5.2295e-01,\n",
            "            1.1475e+00, -1.1123e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 5.5420e-01, -8.8330e-01,  2.6636e-01,  ..., -5.5273e-01,\n",
            "            6.2451e-01,  1.0371e+00],\n",
            "          [... -6.7578e-01,\n",
            "           -6.5088e-01, -6.9043e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-2.7612e-01, -1.2031e+00,  7.4658e-01,  ..., -1.2822e+00,\n",
            "           -5.6836e-01, -1.7615e-01],\n",
            "          [... -8.8232e-01,\n",
            "            3.9453e-01,  1.1338e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07216878364870322, causal = True, window_size = tensor([44, 39])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________ test_flash_attn_qkvpacked[0.0-200-224-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 224, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -4.6655e-01,\n",
            "            2.3301e+00,  5.5664e-01],\n",
            "          [... -2.4365e-01,\n",
            "           -6.6797e-01,  6.2109e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 1.1945e-01,  1.2480e+00,  9.8584e-01,  ...,  1.0273e+00,\n",
            "           -7.3509e-03, -4.2261e-01],\n",
            "          [... -5.8740e-01,\n",
            "           -1.0166e+00, -1.3293e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 6.2939e-01,  9.9316e-01,  9.0771e-01,  ...,  1.7930e+00,\n",
            "            1.0527e+00, -1.2305e+00],\n",
            "          [... -2.4297e+00,\n",
            "            1.2832e+00, -1.3164e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.0668153104781061, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-224-False-True-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 224, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -4.6655e-01,\n",
            "            2.3301e+00,  5.5664e-01],\n",
            "          [... -2.4365e-01,\n",
            "           -6.6797e-01,  6.2109e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 1.1945e-01,  1.2480e+00,  9.8584e-01,  ...,  1.0273e+00,\n",
            "           -7.3509e-03, -4.2261e-01],\n",
            "          [... -5.8740e-01,\n",
            "           -1.0166e+00, -1.3293e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 6.2939e-01,  9.9316e-01,  9.0771e-01,  ...,  1.7930e+00,\n",
            "            1.0527e+00, -1.2305e+00],\n",
            "          [... -2.4297e+00,\n",
            "            1.2832e+00, -1.3164e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.0668153104781061, causal = False, window_size = tensor([44, 39])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-224-True-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 224, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -4.6655e-01,\n",
            "            2.3301e+00,  5.5664e-01],\n",
            "          [... -2.4365e-01,\n",
            "           -6.6797e-01,  6.2109e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 1.1945e-01,  1.2480e+00,  9.8584e-01,  ...,  1.0273e+00,\n",
            "           -7.3509e-03, -4.2261e-01],\n",
            "          [... -5.8740e-01,\n",
            "           -1.0166e+00, -1.3293e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 6.2939e-01,  9.9316e-01,  9.0771e-01,  ...,  1.7930e+00,\n",
            "            1.0527e+00, -1.2305e+00],\n",
            "          [... -2.4297e+00,\n",
            "            1.2832e+00, -1.3164e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.0668153104781061, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-224-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 224, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -4.6655e-01,\n",
            "            2.3301e+00,  5.5664e-01],\n",
            "          [... -2.4365e-01,\n",
            "           -6.6797e-01,  6.2109e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 1.1945e-01,  1.2480e+00,  9.8584e-01,  ...,  1.0273e+00,\n",
            "           -7.3509e-03, -4.2261e-01],\n",
            "          [... -5.8740e-01,\n",
            "           -1.0166e+00, -1.3293e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 6.2939e-01,  9.9316e-01,  9.0771e-01,  ...,  1.7930e+00,\n",
            "            1.0527e+00, -1.2305e+00],\n",
            "          [... -2.4297e+00,\n",
            "            1.2832e+00, -1.3164e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.0668153104781061, causal = True, window_size = tensor([44, 39])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________ test_flash_attn_qkvpacked[0.0-200-256-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 256, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.3418e+00,\n",
            "            3.3164e+00, -8.4668e-01],\n",
            "          [... -1.4441e-01,\n",
            "            2.4473e+00,  5.1562e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 7.0850e-01, -5.7471e-01,  8.4229e-01,  ..., -1.7456e-01,\n",
            "           -9.5459e-01, -6.7688e-02],\n",
            "          [...  7.8613e-01,\n",
            "           -2.1448e-01, -1.4795e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 8.3301e-01, -2.6445e+00, -1.1334e-01,  ...,  5.3516e-01,\n",
            "           -4.7241e-01, -8.3301e-01],\n",
            "          [...  1.3438e+00,\n",
            "           -5.1611e-01,  7.5977e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.0625, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-256-False-True-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 256, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.3418e+00,\n",
            "            3.3164e+00, -8.4668e-01],\n",
            "          [... -1.4441e-01,\n",
            "            2.4473e+00,  5.1562e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 7.0850e-01, -5.7471e-01,  8.4229e-01,  ..., -1.7456e-01,\n",
            "           -9.5459e-01, -6.7688e-02],\n",
            "          [...  7.8613e-01,\n",
            "           -2.1448e-01, -1.4795e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 8.3301e-01, -2.6445e+00, -1.1334e-01,  ...,  5.3516e-01,\n",
            "           -4.7241e-01, -8.3301e-01],\n",
            "          [...  1.3438e+00,\n",
            "           -5.1611e-01,  7.5977e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.0625, causal = False, window_size = tensor([44, 39])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-256-True-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 256, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.3418e+00,\n",
            "            3.3164e+00, -8.4668e-01],\n",
            "          [... -1.4441e-01,\n",
            "            2.4473e+00,  5.1562e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 7.0850e-01, -5.7471e-01,  8.4229e-01,  ..., -1.7456e-01,\n",
            "           -9.5459e-01, -6.7688e-02],\n",
            "          [...  7.8613e-01,\n",
            "           -2.1448e-01, -1.4795e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 8.3301e-01, -2.6445e+00, -1.1334e-01,  ...,  5.3516e-01,\n",
            "           -4.7241e-01, -8.3301e-01],\n",
            "          [...  1.3438e+00,\n",
            "           -5.1611e-01,  7.5977e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.0625, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-200-256-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 200, d = 256, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.3418e+00,\n",
            "            3.3164e+00, -8.4668e-01],\n",
            "          [... -1.4441e-01,\n",
            "            2.4473e+00,  5.1562e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 7.0850e-01, -5.7471e-01,  8.4229e-01,  ..., -1.7456e-01,\n",
            "           -9.5459e-01, -6.7688e-02],\n",
            "          [...  7.8613e-01,\n",
            "           -2.1448e-01, -1.4795e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 8.3301e-01, -2.6445e+00, -1.1334e-01,  ...,  5.3516e-01,\n",
            "           -4.7241e-01, -8.3301e-01],\n",
            "          [...  1.3438e+00,\n",
            "           -5.1611e-01,  7.5977e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.0625, causal = True, window_size = tensor([44, 39])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-32-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 32, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...  9.8340e-01,\n",
            "            2.4785e+00,  9.4043e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [... -9.8291e-01,\n",
            "            8.8086e-01,  2.5469e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [... -1.2021e+00,\n",
            "           -2.4780e-01, -1.9043e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-32-False-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 32, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...  9.8340e-01,\n",
            "            2.4785e+00,  9.4043e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [... -9.8291e-01,\n",
            "            8.8086e-01,  2.5469e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [... -1.2021e+00,\n",
            "           -2.4780e-01, -1.9043e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = False\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-32-True-False-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 32, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...  9.8340e-01,\n",
            "            2.4785e+00,  9.4043e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [... -9.8291e-01,\n",
            "            8.8086e-01,  2.5469e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [... -1.2021e+00,\n",
            "           -2.4780e-01, -1.9043e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m______________________ test_flash_attn_qkvpacked[0.0-256-32-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 32, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...  9.8340e-01,\n",
            "            2.4785e+00,  9.4043e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [... -9.8291e-01,\n",
            "            8.8086e-01,  2.5469e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [... -1.2021e+00,\n",
            "           -2.4780e-01, -1.9043e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = True, window_size = tensor([172,  47])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-40-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 40, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [... -3.1689e-01,\n",
            "            8.4814e-01,  1.7939e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...  9.5801e-01,\n",
            "            3.5718e-01, -8.1329e-03]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...  1.7656e+00,\n",
            "            2.6978e-01, -4.5654e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-40-False-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 40, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [... -3.1689e-01,\n",
            "            8.4814e-01,  1.7939e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...  9.5801e-01,\n",
            "            3.5718e-01, -8.1329e-03]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...  1.7656e+00,\n",
            "            2.6978e-01, -4.5654e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = False\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-40-True-False-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 40, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [... -3.1689e-01,\n",
            "            8.4814e-01,  1.7939e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...  9.5801e-01,\n",
            "            3.5718e-01, -8.1329e-03]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...  1.7656e+00,\n",
            "            2.6978e-01, -4.5654e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m______________________ test_flash_attn_qkvpacked[0.0-256-40-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 40, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [... -3.1689e-01,\n",
            "            8.4814e-01,  1.7939e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...  9.5801e-01,\n",
            "            3.5718e-01, -8.1329e-03]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...  1.7656e+00,\n",
            "            2.6978e-01, -4.5654e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = True\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-59-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 59, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.4980e-01,\n",
            "           -3.2300e-01, -5.4688e-01],\n",
            "          [... -3.8116e-02,\n",
            "           -7.1533e-01,  1.5557e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 8.5107e-01,  4.1919e-01, -3.4644e-01,  ...,  3.6035e-01,\n",
            "           -7.6416e-01, -9.5020e-01],\n",
            "          [...  2.7109e+00,\n",
            "            4.0649e-01,  4.7095e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 1.5400e+00,  2.7173e-01, -1.2051e+00,  ..., -3.0957e-01,\n",
            "           -9.6802e-02, -1.0850e+00],\n",
            "          [... -9.1211e-01,\n",
            "            5.6543e-01, -8.4400e-05]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.13018891098082386, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-59-False-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 59, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.4980e-01,\n",
            "           -3.2300e-01, -5.4688e-01],\n",
            "          [... -3.8116e-02,\n",
            "           -7.1533e-01,  1.5557e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 8.5107e-01,  4.1919e-01, -3.4644e-01,  ...,  3.6035e-01,\n",
            "           -7.6416e-01, -9.5020e-01],\n",
            "          [...  2.7109e+00,\n",
            "            4.0649e-01,  4.7095e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 1.5400e+00,  2.7173e-01, -1.2051e+00,  ..., -3.0957e-01,\n",
            "           -9.6802e-02, -1.0850e+00],\n",
            "          [... -9.1211e-01,\n",
            "            5.6543e-01, -8.4400e-05]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.13018891098082386, causal = False\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-59-True-False-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 59, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.4980e-01,\n",
            "           -3.2300e-01, -5.4688e-01],\n",
            "          [... -3.8116e-02,\n",
            "           -7.1533e-01,  1.5557e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 8.5107e-01,  4.1919e-01, -3.4644e-01,  ...,  3.6035e-01,\n",
            "           -7.6416e-01, -9.5020e-01],\n",
            "          [...  2.7109e+00,\n",
            "            4.0649e-01,  4.7095e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 1.5400e+00,  2.7173e-01, -1.2051e+00,  ..., -3.0957e-01,\n",
            "           -9.6802e-02, -1.0850e+00],\n",
            "          [... -9.1211e-01,\n",
            "            5.6543e-01, -8.4400e-05]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.13018891098082386, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m______________________ test_flash_attn_qkvpacked[0.0-256-59-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 59, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.4980e-01,\n",
            "           -3.2300e-01, -5.4688e-01],\n",
            "          [... -3.8116e-02,\n",
            "           -7.1533e-01,  1.5557e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 8.5107e-01,  4.1919e-01, -3.4644e-01,  ...,  3.6035e-01,\n",
            "           -7.6416e-01, -9.5020e-01],\n",
            "          [...  2.7109e+00,\n",
            "            4.0649e-01,  4.7095e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 1.5400e+00,  2.7173e-01, -1.2051e+00,  ..., -3.0957e-01,\n",
            "           -9.6802e-02, -1.0850e+00],\n",
            "          [... -9.1211e-01,\n",
            "            5.6543e-01, -8.4400e-05]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.13018891098082386, causal = True\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-64-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 64, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.2103e-01,\n",
            "            4.7290e-01, -1.0820e+00],\n",
            "          [... -2.9102e-01,\n",
            "            2.3083e-01,  1.2363e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -3.4521e-01,\n",
            "            1.3809e+00,  7.7832e-01],\n",
            "          [... -5.3809e-01,\n",
            "           -7.5049e-01, -1.6094e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 7.3975e-01, -8.1592e-01,  2.2852e-01,  ...,  1.5342e+00,\n",
            "            7.5293e-01,  1.2109e+00],\n",
            "          [...  5.3467e-02,\n",
            "           -1.4883e+00, -1.3066e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.125, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-64-False-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 64, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.2103e-01,\n",
            "            4.7290e-01, -1.0820e+00],\n",
            "          [... -2.9102e-01,\n",
            "            2.3083e-01,  1.2363e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -3.4521e-01,\n",
            "            1.3809e+00,  7.7832e-01],\n",
            "          [... -5.3809e-01,\n",
            "           -7.5049e-01, -1.6094e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 7.3975e-01, -8.1592e-01,  2.2852e-01,  ...,  1.5342e+00,\n",
            "            7.5293e-01,  1.2109e+00],\n",
            "          [...  5.3467e-02,\n",
            "           -1.4883e+00, -1.3066e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.125, causal = False, window_size = tensor([172,  47])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-64-True-False-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 64, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.2103e-01,\n",
            "            4.7290e-01, -1.0820e+00],\n",
            "          [... -2.9102e-01,\n",
            "            2.3083e-01,  1.2363e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -3.4521e-01,\n",
            "            1.3809e+00,  7.7832e-01],\n",
            "          [... -5.3809e-01,\n",
            "           -7.5049e-01, -1.6094e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 7.3975e-01, -8.1592e-01,  2.2852e-01,  ...,  1.5342e+00,\n",
            "            7.5293e-01,  1.2109e+00],\n",
            "          [...  5.3467e-02,\n",
            "           -1.4883e+00, -1.3066e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.125, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m______________________ test_flash_attn_qkvpacked[0.0-256-64-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 64, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.2103e-01,\n",
            "            4.7290e-01, -1.0820e+00],\n",
            "          [... -2.9102e-01,\n",
            "            2.3083e-01,  1.2363e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -3.4521e-01,\n",
            "            1.3809e+00,  7.7832e-01],\n",
            "          [... -5.3809e-01,\n",
            "           -7.5049e-01, -1.6094e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 7.3975e-01, -8.1592e-01,  2.2852e-01,  ...,  1.5342e+00,\n",
            "            7.5293e-01,  1.2109e+00],\n",
            "          [...  5.3467e-02,\n",
            "           -1.4883e+00, -1.3066e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.125, causal = True, window_size = tensor([172,  47])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-80-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 80, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -4.3579e-02,\n",
            "            3.5962e-01, -5.0439e-01],\n",
            "          [... -7.4316e-01,\n",
            "           -1.4570e+00,  3.8770e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ..., -4.4751e-01,\n",
            "            7.7576e-02,  1.3147e-01],\n",
            "          [...  8.0078e-02,\n",
            "            1.5938e+00, -1.3008e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.4443e+00,  2.9565e-01, -2.4976e-01,  ...,  3.1421e-01,\n",
            "            5.9229e-01, -7.7930e-01],\n",
            "          [...  6.5674e-02,\n",
            "           -3.2666e-01, -6.4990e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.11180339887498948, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-80-False-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 80, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -4.3579e-02,\n",
            "            3.5962e-01, -5.0439e-01],\n",
            "          [... -7.4316e-01,\n",
            "           -1.4570e+00,  3.8770e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ..., -4.4751e-01,\n",
            "            7.7576e-02,  1.3147e-01],\n",
            "          [...  8.0078e-02,\n",
            "            1.5938e+00, -1.3008e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.4443e+00,  2.9565e-01, -2.4976e-01,  ...,  3.1421e-01,\n",
            "            5.9229e-01, -7.7930e-01],\n",
            "          [...  6.5674e-02,\n",
            "           -3.2666e-01, -6.4990e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.11180339887498948, causal = False\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-80-True-False-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 80, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -4.3579e-02,\n",
            "            3.5962e-01, -5.0439e-01],\n",
            "          [... -7.4316e-01,\n",
            "           -1.4570e+00,  3.8770e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ..., -4.4751e-01,\n",
            "            7.7576e-02,  1.3147e-01],\n",
            "          [...  8.0078e-02,\n",
            "            1.5938e+00, -1.3008e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.4443e+00,  2.9565e-01, -2.4976e-01,  ...,  3.1421e-01,\n",
            "            5.9229e-01, -7.7930e-01],\n",
            "          [...  6.5674e-02,\n",
            "           -3.2666e-01, -6.4990e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.11180339887498948, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m______________________ test_flash_attn_qkvpacked[0.0-256-80-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 80, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -4.3579e-02,\n",
            "            3.5962e-01, -5.0439e-01],\n",
            "          [... -7.4316e-01,\n",
            "           -1.4570e+00,  3.8770e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ..., -4.4751e-01,\n",
            "            7.7576e-02,  1.3147e-01],\n",
            "          [...  8.0078e-02,\n",
            "            1.5938e+00, -1.3008e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.4443e+00,  2.9565e-01, -2.4976e-01,  ...,  3.1421e-01,\n",
            "            5.9229e-01, -7.7930e-01],\n",
            "          [...  6.5674e-02,\n",
            "           -3.2666e-01, -6.4990e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.11180339887498948, causal = True\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-96-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 96, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  5.2441e-01,\n",
            "            1.1699e+00, -9.5557e-01],\n",
            "          [...  9.6240e-01,\n",
            "            1.6248e-01, -1.3977e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-3.4717e-01, -1.6543e+00,  5.0830e-01,  ..., -2.1699e+00,\n",
            "           -1.0420e+00, -9.6533e-01],\n",
            "          [...  8.7097e-02,\n",
            "           -3.8116e-02, -1.0566e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 5.5420e-01, -8.8330e-01,  2.6636e-01,  ...,  3.3252e-01,\n",
            "           -5.5811e-01, -8.7109e-01],\n",
            "          [...  1.0977e+00,\n",
            "           -9.4385e-01, -1.4482e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.10206207261596575, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-96-False-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 96, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  5.2441e-01,\n",
            "            1.1699e+00, -9.5557e-01],\n",
            "          [...  9.6240e-01,\n",
            "            1.6248e-01, -1.3977e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-3.4717e-01, -1.6543e+00,  5.0830e-01,  ..., -2.1699e+00,\n",
            "           -1.0420e+00, -9.6533e-01],\n",
            "          [...  8.7097e-02,\n",
            "           -3.8116e-02, -1.0566e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 5.5420e-01, -8.8330e-01,  2.6636e-01,  ...,  3.3252e-01,\n",
            "           -5.5811e-01, -8.7109e-01],\n",
            "          [...  1.0977e+00,\n",
            "           -9.4385e-01, -1.4482e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.10206207261596575, causal = False\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-96-True-False-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 96, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  5.2441e-01,\n",
            "            1.1699e+00, -9.5557e-01],\n",
            "          [...  9.6240e-01,\n",
            "            1.6248e-01, -1.3977e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-3.4717e-01, -1.6543e+00,  5.0830e-01,  ..., -2.1699e+00,\n",
            "           -1.0420e+00, -9.6533e-01],\n",
            "          [...  8.7097e-02,\n",
            "           -3.8116e-02, -1.0566e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 5.5420e-01, -8.8330e-01,  2.6636e-01,  ...,  3.3252e-01,\n",
            "           -5.5811e-01, -8.7109e-01],\n",
            "          [...  1.0977e+00,\n",
            "           -9.4385e-01, -1.4482e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.10206207261596575, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m______________________ test_flash_attn_qkvpacked[0.0-256-96-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 96, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  5.2441e-01,\n",
            "            1.1699e+00, -9.5557e-01],\n",
            "          [...  9.6240e-01,\n",
            "            1.6248e-01, -1.3977e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-3.4717e-01, -1.6543e+00,  5.0830e-01,  ..., -2.1699e+00,\n",
            "           -1.0420e+00, -9.6533e-01],\n",
            "          [...  8.7097e-02,\n",
            "           -3.8116e-02, -1.0566e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 5.5420e-01, -8.8330e-01,  2.6636e-01,  ...,  3.3252e-01,\n",
            "           -5.5811e-01, -8.7109e-01],\n",
            "          [...  1.0977e+00,\n",
            "           -9.4385e-01, -1.4482e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.10206207261596575, causal = True\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________ test_flash_attn_qkvpacked[0.0-256-111-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 111, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  6.5869e-01,\n",
            "           -3.5547e-01, -1.9111e+00],\n",
            "          [...  2.3223e+00,\n",
            "            8.3447e-01,  1.9102e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 2.2695e+00, -3.8940e-01,  1.2666e+00,  ..., -5.9424e-01,\n",
            "           -3.2031e-01,  2.0176e+00],\n",
            "          [...  7.4219e-02,\n",
            "           -1.3076e+00, -3.6841e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.0186e+00, -4.6436e-01,  1.2949e+00,  ..., -8.7341e-02,\n",
            "            3.5205e-01, -1.3994e+00],\n",
            "          [...  9.0332e-02,\n",
            "            2.1816e+00,  8.8037e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.0949157995752499, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-111-False-True-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 111, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  6.5869e-01,\n",
            "           -3.5547e-01, -1.9111e+00],\n",
            "          [...  2.3223e+00,\n",
            "            8.3447e-01,  1.9102e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 2.2695e+00, -3.8940e-01,  1.2666e+00,  ..., -5.9424e-01,\n",
            "           -3.2031e-01,  2.0176e+00],\n",
            "          [...  7.4219e-02,\n",
            "           -1.3076e+00, -3.6841e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.0186e+00, -4.6436e-01,  1.2949e+00,  ..., -8.7341e-02,\n",
            "            3.5205e-01, -1.3994e+00],\n",
            "          [...  9.0332e-02,\n",
            "            2.1816e+00,  8.8037e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.0949157995752499, causal = False\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-111-True-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 111, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  6.5869e-01,\n",
            "           -3.5547e-01, -1.9111e+00],\n",
            "          [...  2.3223e+00,\n",
            "            8.3447e-01,  1.9102e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 2.2695e+00, -3.8940e-01,  1.2666e+00,  ..., -5.9424e-01,\n",
            "           -3.2031e-01,  2.0176e+00],\n",
            "          [...  7.4219e-02,\n",
            "           -1.3076e+00, -3.6841e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.0186e+00, -4.6436e-01,  1.2949e+00,  ..., -8.7341e-02,\n",
            "            3.5205e-01, -1.3994e+00],\n",
            "          [...  9.0332e-02,\n",
            "            2.1816e+00,  8.8037e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.0949157995752499, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-111-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 111, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  6.5869e-01,\n",
            "           -3.5547e-01, -1.9111e+00],\n",
            "          [...  2.3223e+00,\n",
            "            8.3447e-01,  1.9102e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 2.2695e+00, -3.8940e-01,  1.2666e+00,  ..., -5.9424e-01,\n",
            "           -3.2031e-01,  2.0176e+00],\n",
            "          [...  7.4219e-02,\n",
            "           -1.3076e+00, -3.6841e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.0186e+00, -4.6436e-01,  1.2949e+00,  ..., -8.7341e-02,\n",
            "            3.5205e-01, -1.3994e+00],\n",
            "          [...  9.0332e-02,\n",
            "            2.1816e+00,  8.8037e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.0949157995752499, causal = True, window_size = tensor([172,  47])\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________ test_flash_attn_qkvpacked[0.0-256-128-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 128, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -2.1277e-01,\n",
            "           -3.3154e-01, -2.0227e-01],\n",
            "          [...  8.2422e-01,\n",
            "           -6.4062e-01, -4.0381e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 7.3975e-01, -8.1592e-01,  2.2852e-01,  ..., -5.4248e-01,\n",
            "            3.7903e-02,  6.7725e-01],\n",
            "          [... -6.3965e-01,\n",
            "            6.6357e-01, -3.0869e-02]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 7.0850e-01, -5.7471e-01,  8.4229e-01,  ..., -2.6660e-01,\n",
            "           -1.7461e+00,  1.4756e+00],\n",
            "          [...  7.9688e-01,\n",
            "            1.0126e-01,  5.8496e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.08838834764831845, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-128-False-True-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 128, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -2.1277e-01,\n",
            "           -3.3154e-01, -2.0227e-01],\n",
            "          [...  8.2422e-01,\n",
            "           -6.4062e-01, -4.0381e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 7.3975e-01, -8.1592e-01,  2.2852e-01,  ..., -5.4248e-01,\n",
            "            3.7903e-02,  6.7725e-01],\n",
            "          [... -6.3965e-01,\n",
            "            6.6357e-01, -3.0869e-02]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 7.0850e-01, -5.7471e-01,  8.4229e-01,  ..., -2.6660e-01,\n",
            "           -1.7461e+00,  1.4756e+00],\n",
            "          [...  7.9688e-01,\n",
            "            1.0126e-01,  5.8496e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.08838834764831845, causal = False\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-128-True-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 128, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -2.1277e-01,\n",
            "           -3.3154e-01, -2.0227e-01],\n",
            "          [...  8.2422e-01,\n",
            "           -6.4062e-01, -4.0381e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 7.3975e-01, -8.1592e-01,  2.2852e-01,  ..., -5.4248e-01,\n",
            "            3.7903e-02,  6.7725e-01],\n",
            "          [... -6.3965e-01,\n",
            "            6.6357e-01, -3.0869e-02]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 7.0850e-01, -5.7471e-01,  8.4229e-01,  ..., -2.6660e-01,\n",
            "           -1.7461e+00,  1.4756e+00],\n",
            "          [...  7.9688e-01,\n",
            "            1.0126e-01,  5.8496e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.08838834764831845, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-128-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 128, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -2.1277e-01,\n",
            "           -3.3154e-01, -2.0227e-01],\n",
            "          [...  8.2422e-01,\n",
            "           -6.4062e-01, -4.0381e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 7.3975e-01, -8.1592e-01,  2.2852e-01,  ..., -5.4248e-01,\n",
            "            3.7903e-02,  6.7725e-01],\n",
            "          [... -6.3965e-01,\n",
            "            6.6357e-01, -3.0869e-02]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[ 7.0850e-01, -5.7471e-01,  8.4229e-01,  ..., -2.6660e-01,\n",
            "           -1.7461e+00,  1.4756e+00],\n",
            "          [...  7.9688e-01,\n",
            "            1.0126e-01,  5.8496e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.08838834764831845, causal = True\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________ test_flash_attn_qkvpacked[0.0-256-160-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 160, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.2852e+00,\n",
            "            7.5732e-01, -8.3154e-01],\n",
            "          [... -1.6387e+00,\n",
            "           -3.0811e-01, -8.0811e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-1.4443e+00,  2.9565e-01, -2.4976e-01,  ...,  5.6299e-01,\n",
            "            2.9443e-01, -1.0242e-01],\n",
            "          [...  1.9141e-01,\n",
            "            3.4180e-01, -9.2285e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.2002e+00,  1.6396e+00, -4.3915e-02,  ..., -7.9785e-01,\n",
            "           -2.2363e-01, -4.7534e-01],\n",
            "          [... -2.8638e-01,\n",
            "            1.8398e+00, -6.1035e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07905694150420949, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-160-False-True-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 160, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.2852e+00,\n",
            "            7.5732e-01, -8.3154e-01],\n",
            "          [... -1.6387e+00,\n",
            "           -3.0811e-01, -8.0811e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-1.4443e+00,  2.9565e-01, -2.4976e-01,  ...,  5.6299e-01,\n",
            "            2.9443e-01, -1.0242e-01],\n",
            "          [...  1.9141e-01,\n",
            "            3.4180e-01, -9.2285e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.2002e+00,  1.6396e+00, -4.3915e-02,  ..., -7.9785e-01,\n",
            "           -2.2363e-01, -4.7534e-01],\n",
            "          [... -2.8638e-01,\n",
            "            1.8398e+00, -6.1035e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07905694150420949, causal = False\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-160-True-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 160, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.2852e+00,\n",
            "            7.5732e-01, -8.3154e-01],\n",
            "          [... -1.6387e+00,\n",
            "           -3.0811e-01, -8.0811e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-1.4443e+00,  2.9565e-01, -2.4976e-01,  ...,  5.6299e-01,\n",
            "            2.9443e-01, -1.0242e-01],\n",
            "          [...  1.9141e-01,\n",
            "            3.4180e-01, -9.2285e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.2002e+00,  1.6396e+00, -4.3915e-02,  ..., -7.9785e-01,\n",
            "           -2.2363e-01, -4.7534e-01],\n",
            "          [... -2.8638e-01,\n",
            "            1.8398e+00, -6.1035e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07905694150420949, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-160-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 160, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  1.2852e+00,\n",
            "            7.5732e-01, -8.3154e-01],\n",
            "          [... -1.6387e+00,\n",
            "           -3.0811e-01, -8.0811e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[-1.4443e+00,  2.9565e-01, -2.4976e-01,  ...,  5.6299e-01,\n",
            "            2.9443e-01, -1.0242e-01],\n",
            "          [...  1.9141e-01,\n",
            "            3.4180e-01, -9.2285e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-1.2002e+00,  1.6396e+00, -4.3915e-02,  ..., -7.9785e-01,\n",
            "           -2.2363e-01, -4.7534e-01],\n",
            "          [... -2.8638e-01,\n",
            "            1.8398e+00, -6.1035e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07905694150420949, causal = True\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________ test_flash_attn_qkvpacked[0.0-256-192-False-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 192, dropout_p = 0.0, causal = False, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  2.1448e-01,\n",
            "           -7.3682e-01, -4.5166e-01],\n",
            "          [...  1.0645e+00,\n",
            "            4.0054e-03,  1.9756e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 5.5420e-01, -8.8330e-01,  2.6636e-01,  ..., -5.5273e-01,\n",
            "            6.2451e-01,  1.0371e+00],\n",
            "          [...  1.2363e+00,\n",
            "           -4.8853e-01, -3.2544e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-2.7612e-01, -1.2031e+00,  7.4658e-01,  ..., -1.2822e+00,\n",
            "           -5.6836e-01, -1.7615e-01],\n",
            "          [... -3.6548e-01,\n",
            "           -7.1631e-01,  4.1260e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07216878364870322, causal = False, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-192-False-True-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 192, dropout_p = 0.0, causal = False, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  2.1448e-01,\n",
            "           -7.3682e-01, -4.5166e-01],\n",
            "          [...  1.0645e+00,\n",
            "            4.0054e-03,  1.9756e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 5.5420e-01, -8.8330e-01,  2.6636e-01,  ..., -5.5273e-01,\n",
            "            6.2451e-01,  1.0371e+00],\n",
            "          [...  1.2363e+00,\n",
            "           -4.8853e-01, -3.2544e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-2.7612e-01, -1.2031e+00,  7.4658e-01,  ..., -1.2822e+00,\n",
            "           -5.6836e-01, -1.7615e-01],\n",
            "          [... -3.6548e-01,\n",
            "           -7.1631e-01,  4.1260e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07216878364870322, causal = False\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-192-True-False-dtype0] _____________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 192, dropout_p = 0.0, causal = True, local = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  2.1448e-01,\n",
            "           -7.3682e-01, -4.5166e-01],\n",
            "          [...  1.0645e+00,\n",
            "            4.0054e-03,  1.9756e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 5.5420e-01, -8.8330e-01,  2.6636e-01,  ..., -5.5273e-01,\n",
            "            6.2451e-01,  1.0371e+00],\n",
            "          [...  1.2363e+00,\n",
            "           -4.8853e-01, -3.2544e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-2.7612e-01, -1.2031e+00,  7.4658e-01,  ..., -1.2822e+00,\n",
            "           -5.6836e-01, -1.7615e-01],\n",
            "          [... -3.6548e-01,\n",
            "           -7.1631e-01,  4.1260e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07216878364870322, causal = True, window_size = (-1, -1)\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_flash_attn_forward\u001b[39;49;00m(q, k, v, dropout_p, softmax_scale, causal, window_size, return_softmax):\u001b[90m\u001b[39;49;00m\n",
            "        maybe_contiguous = \u001b[94mlambda\u001b[39;49;00m x: x.contiguous() \u001b[94mif\u001b[39;49;00m x.stride(-\u001b[94m1\u001b[39;49;00m) != \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m x\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            window_size[\u001b[94m1\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:47: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_flash_attn_qkvpacked[0.0-256-192-True-True-dtype0] ______________________\u001b[0m\n",
            "\n",
            "seqlen = 256, d = 192, dropout_p = 0.0, causal = True, local = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [True])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m, \u001b[94m257\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m512\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m13\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv, dropout_p, causal=causal, window_size=window_size, return_attn_probs=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_flash_attn.py\u001b[0m:557: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:588: in flash_attn_qkvpacked_func\n",
            "    \u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m:539: in apply\n",
            "    \u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:198: in forward\n",
            "    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ...,  2.1448e-01,\n",
            "           -7.3682e-01, -4.5166e-01],\n",
            "          [...  1.0645e+00,\n",
            "            4.0054e-03,  1.9756e+00]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "k = tensor([[[[ 5.5420e-01, -8.8330e-01,  2.6636e-01,  ..., -5.5273e-01,\n",
            "            6.2451e-01,  1.0371e+00],\n",
            "          [...  1.2363e+00,\n",
            "           -4.8853e-01, -3.2544e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "v = tensor([[[[-2.7612e-01, -1.2031e+00,  7.4658e-01,  ..., -1.2822e+00,\n",
            "           -5.6836e-01, -1.7615e-01],\n",
            "          [... -3.6548e-01,\n",
            "           -7.1631e-01,  4.1260e-01]]]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "dropout_p = 0.0, softmax_scale = 0.07216878364870322, causal = True\n",
            "window_size = tensor([172,  47]), return_softmax = False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pytest -q -s tests/test_flash_attn.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VKXL3-tZZLV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}