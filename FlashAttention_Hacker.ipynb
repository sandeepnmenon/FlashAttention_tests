{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeepnmenon/FlashAttention_tests/blob/master/FlashAttention_Hacker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhdoo7eCaKMK",
        "outputId": "d1cff2b9-4760-4728-a199-093727a82013"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.3.4.tar.gz (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.1.0+cu118)\n",
            "Collecting einops (from flash-attn)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (23.2)\n",
            "Collecting ninja (from flash-attn)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.3.4-cp310-cp310-linux_x86_64.whl size=57449248 sha256=4ceff7f9ad6a6747975b568f3193b2416f598808b5e860fb09555dbfa32b42cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/19/1b/bf8147431c43f8b8d2b2e5d66be1ebf586f2fca96d72203f53\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: ninja, einops, flash-attn\n",
            "Successfully installed einops-0.7.0 flash-attn-2.3.4 ninja-1.11.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0KEIvTL_kaF",
        "outputId": "7c3fd106-7a33-4704-874d-1a78adaec61c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from flash_attn import flash_attn_qkvpacked_func, flash_attn_func\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the basic parameters for the model\n",
        "batch_size = 32\n",
        "sequence_length = 2048\n",
        "dimensions = 64\n",
        "number_of_heads = 8\n",
        "\n",
        "# Define the dropout rate and number of trials for benchmarking\n",
        "dropout_rate = 0.0\n",
        "num_trials = 10\n"
      ],
      "metadata": {
        "id": "ATM_el16fd83"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time Benchmark"
      ],
      "metadata": {
        "id": "NBMpYmofejiZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUZW57xDBN-T",
        "outputId": "b6cc08f8-9744-4e19-bd57-79b4ac8708c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard attention took 0.09350252151489258 seconds for 10 trials\n",
            "Standard attention took 0.11731982231140137 seconds for 10 trials\n",
            "Flash attention took 0.04467010498046875 seconds for 10 trials\n",
            "Speedup of Flash attention over standard attention: 2.63x\n",
            "FlashAttention V2 took 0.01967453956604004 seconds for 10 trials\n",
            "Speedup of Flash attention over standard attention: 5.96x\n",
            "FlashAttention QKV Packed took 0.018848896026611328 seconds for 10 trials\n",
            "Speedup of Flash attention over standard attention: 6.22x\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Creating query (q), key (k), and value (v) tensors\n",
        "# These tensors are initialized with random values and moved to the GPU for faster processing\n",
        "q = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "k = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "v = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "\n",
        "\n",
        "# Standard Attention Computation\n",
        "torch.cuda.synchronize()  # Synchronizes CPU and GPU to ensure accurate timing\n",
        "start = time.time()  # Start timer\n",
        "for i in range(num_trials):\n",
        "    attn = q @ k.transpose(-2, -1)  # Compute attention scores\n",
        "    attn = attn.softmax(dim=-1)  # Apply softmax to get probabilities\n",
        "    attn = F.dropout(attn, p=dropout_rate, training=True)  # Apply dropout\n",
        "    x = (attn @ v).transpose(1, 2)  # Apply attention to value and reshape\n",
        "torch.cuda.synchronize()  # Ensure all GPU tasks are finished\n",
        "end = time.time()  # End timer\n",
        "standard_attention_time = end - start\n",
        "print('Standard attention took {} seconds for {} trials'.format(standard_attention_time, num_trials))\n",
        "\n",
        "# Standard Attention Computation\n",
        "with torch.backends.cuda.sdp_kernel(\n",
        "    enable_flash=False, enable_math=True, enable_mem_efficient=False\n",
        "):\n",
        "    torch.cuda.synchronize()  # Synchronizes CPU and GPU for accurate timing\n",
        "    start = time.time()  # Start timer\n",
        "    for i in range(num_trials):\n",
        "        out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_rate)  # Compute attention using FlashAttention\n",
        "    torch.cuda.synchronize()  # Ensure completion of all GPU tasks\n",
        "    end = time.time()  # End timer\n",
        "    flash_attention_time = end - start\n",
        "standard_attention_time = end - start\n",
        "print('Standard attention took {} seconds for {} trials'.format(standard_attention_time, num_trials))\n",
        "\n",
        "\n",
        "# Flash Attention Computation\n",
        "with torch.backends.cuda.sdp_kernel(\n",
        "    enable_flash=True, enable_math=False, enable_mem_efficient=False\n",
        "):\n",
        "    torch.cuda.synchronize()  # Synchronizes CPU and GPU for accurate timing\n",
        "    start = time.time()  # Start timer\n",
        "    for i in range(num_trials):\n",
        "        out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_rate)  # Compute attention using FlashAttention\n",
        "    torch.cuda.synchronize()  # Ensure completion of all GPU tasks\n",
        "    end = time.time()  # End timer\n",
        "    flash_attention_time = end - start\n",
        "    print('Flash attention took {} seconds for {} trials'.format(end - start, num_trials))\n",
        "\n",
        "# Speedup for Flash Attention\n",
        "flash_attention_speedup = standard_attention_time / flash_attention_time\n",
        "print('Speedup of Flash attention over standard attention: {:.2f}x'.format(flash_attention_speedup))\n",
        "\n",
        "# FlashAttention V2 Computation (add this to your existing code)\n",
        "torch.cuda.synchronize()  # Synchronizes CPU and GPU for accurate timing\n",
        "start = time.time()  # Start timer\n",
        "for i in range(num_trials):\n",
        "    # Replace 'flash_attn_func' with the actual FlashAttention V2 function if it has a different name\n",
        "    out = flash_attn_func(q, k, v, dropout_p=dropout_rate)  # Compute attention using FlashAttention V2\n",
        "torch.cuda.synchronize()  # Ensure completion of all GPU tasks\n",
        "end = time.time()  # End timer\n",
        "flash_attention_v2_time = end - start\n",
        "print('FlashAttention V2 took {} seconds for {} trials'.format(end - start, num_trials))\n",
        "\n",
        "# Speedup for Flash Attention V2\n",
        "flash_attention_v2_speedup = standard_attention_time / flash_attention_v2_time\n",
        "print('Speedup of Flash attention over standard attention: {:.2f}x'.format(flash_attention_v2_speedup))\n",
        "\n",
        "\n",
        "# FlashAttention QKV Packed Computation (add this to your existing code)\n",
        "torch.cuda.synchronize()  # Synchronizes CPU and GPU for accurate timing\n",
        "# Prepare the qkv tensor by stacking q, k, v along the third dimension\n",
        "qkv = torch.stack((q, k, v), dim=2)\n",
        "start = time.time()  # Start timer\n",
        "for i in range(num_trials):\n",
        "    # Call the flash_attn_qkvpacked_func with the stacked qkv tensor\n",
        "    out= flash_attn_qkvpacked_func(qkv, dropout_p=dropout_rate)\n",
        "torch.cuda.synchronize()  # Ensure completion of all GPU tasks\n",
        "end = time.time()  # End timer\n",
        "flash_attention_v2_time = end - start\n",
        "\n",
        "print('FlashAttention QKV Packed took {} seconds for {} trials'.format(end - start, num_trials))\n",
        "\n",
        "# Speedup for Flash Attention V2 qkv stacked\n",
        "flash_attention_v2_speedup = standard_attention_time / flash_attention_v2_time\n",
        "print('Speedup of Flash attention over standard attention: {:.2f}x'.format(flash_attention_v2_speedup))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flash Attention speedup: 2.94646049738\n",
        "\n",
        "Flash Attention v2 speedup: 11.5634949174\n",
        "\n",
        "Flash Attention v2 QKV speedup: 14.8685056331"
      ],
      "metadata": {
        "id": "ZCOvcclJTdct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVPZE-n6Y6vA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT Inference"
      ],
      "metadata": {
        "id": "rU933mvukjjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/graykode/gpt-2-Pytorch\n",
        "%cd gpt-2-Pytorch\n",
        "!curl --output gpt2-pytorch_model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOy7E_7I07FH",
        "outputId": "5c846283-193a-4818-f0e4-235d4afb7600"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt-2-Pytorch'...\n",
            "remote: Enumerating objects: 130, done.\u001b[K\n",
            "remote: Total 130 (delta 0), reused 0 (delta 0), pack-reused 130\u001b[K\n",
            "Receiving objects: 100% (130/130), 2.39 MiB | 4.70 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n",
            "/content/gpt-2-Pytorch\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  522M  100  522M    0     0  15.0M      0  0:00:34  0:00:34 --:--:-- 16.7M\n",
            "Collecting regex==2017.4.5 (from -r requirements.txt (line 1))\n",
            "  Downloading regex-2017.04.05.tar.gz (601 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.6/601.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp310-cp310-linux_x86_64.whl size=657355 sha256=0359b2e7ffdf39408108783174297daaab7b2e3341765adef480ca36e8addb4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/08/ba/204d631aa07ad5b7ff391f1a4f3e80f56e03cf58f890129a9e\n",
            "Successfully built regex\n",
            "Installing collected packages: regex\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2023.6.3\n",
            "    Uninstalling regex-2023.6.3:\n",
            "      Successfully uninstalled regex-2023.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nltk 3.8.1 requires regex>=2021.8.3, but you have regex 2017.4.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed regex-2017.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# q_h = q.half()\n",
        "# k_h = k.half().transpose(-2,-1)\n",
        "# v_h = v.half()\n",
        "# out = flash_attn_func(q_h, k_h, v_h)\n",
        "# return out"
      ],
      "metadata": {
        "id": "O4Ojyx7or_fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "!python main.py --text \"Once when I was six years old I saw a magnificent picture in a book, called True Stories from Nature, about the primeval forest.\" --length 100\n",
        "inference_time = time.time() - start_time\n",
        "print(f'Inference Time: {inference_time:.3f} seconds')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgYqnAzn2U_i",
        "outputId": "b5d0cddb-024d-47be-cac2-b2a6fca3259b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(text='Once when I was six years old I saw a magnificent picture in a book, called True Stories from Nature, about the primeval forest.', quiet=False, nsamples=1, unconditional=False, batch_size=-1, length=100, temperature=0.7, top_k=40)\n",
            "Once when I was six years old I saw a magnificent picture in a book, called True Stories from Nature, about the primeval forest.\n",
            "100% 100/100 [00:01<00:00, 57.14it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "\n",
            "I thought, 'What's the story about that?' In the book, we're talking about a forest that was once a forest and now is the mainstay of the world, and it's called the 'Lake of Fire.' We're talking about the lake of fire in the book. So when I was seven, I saw a beautiful picture of that forest. I thought, 'Why is this forest a lake of fire? Why did it get so much attention? Why didn't the\n",
            "Inference Time: 5.423 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.eval()\n",
        "model.cuda()  # Move model to GPU\n",
        "\n",
        "# Initial input text\n",
        "input_text = \"Once when I was six years old I saw a magnificent picture in a book, called True Stories from Nature, about the primeval forest.\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').cuda()\n",
        "\n",
        "# Function to generate 100 tokens autoregressively\n",
        "def generate_100_tokens(model, input_ids):\n",
        "    generated_text = input_ids\n",
        "    for _ in range(100):\n",
        "        outputs = model(generated_text)\n",
        "        next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
        "        generated_text = torch.cat([generated_text, next_token], dim=-1)\n",
        "    return generated_text\n",
        "\n",
        "# Measure inference time with FlashAttention\n",
        "with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
        "    torch.cuda.synchronize()\n",
        "    start_time = time.time()\n",
        "    generated_text_flash = generate_100_tokens(model, input_ids)\n",
        "    torch.cuda.synchronize()\n",
        "    flash_inference_time = time.time() - start_time\n",
        "print(f'FlashAttention Inference Time: {flash_inference_time:.3f} seconds')\n"
      ],
      "metadata": {
        "id": "XzschxlqlK37",
        "outputId": "19eefbce-1e37-4188-b509-5018cbbfec24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FlashAttention Inference Time: 1.552 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "speedup = inference_time/flash_inference_time\n",
        "print(\"Speedup: \", speedup)"
      ],
      "metadata": {
        "id": "_PnHAFZOlrRL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5125fc2f-d52d-433f-a151-f00e5613afe2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speedup:  3.495269219817352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nrqr251Ytsqx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}