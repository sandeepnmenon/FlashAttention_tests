{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeepnmenon/FlashAttention_tests/blob/master/FlashAttention_Hacker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhdoo7eCaKMK",
        "outputId": "dfa6a96d-3805-47d3-ab19-1009311fcdfc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.3.3.tar.gz (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.1.0+cu118)\n",
            "Collecting einops (from flash-attn)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (23.2)\n",
            "Collecting ninja (from flash-attn)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.3.3-cp310-cp310-linux_x86_64.whl size=57075008 sha256=bcb63b64213ab61590b340b77de84e448a442e19c100480895194df39ad7673d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/e6/fa/941802ec61d1afd320d27160ab1db98e6dba65381f84b76d4a\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: ninja, einops, flash-attn\n",
            "Successfully installed einops-0.7.0 flash-attn-2.3.3 ninja-1.11.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0KEIvTL_kaF",
        "outputId": "0d525575-3208-4b7e-8b51-110a1e81de73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from flash_attn import flash_attn_qkvpacked_func, flash_attn_func\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the basic parameters for the model\n",
        "batch_size = 32\n",
        "sequence_length = 2048\n",
        "dimensions = 64\n",
        "number_of_heads = 8\n",
        "\n",
        "# Define the dropout rate and number of trials for benchmarking\n",
        "dropout_rate = 0.0\n",
        "num_trials = 10\n"
      ],
      "metadata": {
        "id": "ATM_el16fd83"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FlashAttention might be flashy but it is not an Approximation"
      ],
      "metadata": {
        "id": "IiAJNYECemm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a single set of random q, k, v tensors\n",
        "q_single = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "k_single = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "v_single = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "qkv_single = torch.stack((q_single, k_single, v_single), dim=2)\n",
        "\n",
        "# Standard Attention Computation\n",
        "attn = q_single @ k_single.transpose(-2, -1)  # Compute attention scores\n",
        "attn = attn.softmax(dim=-1)  # Apply softmax to get probabilities\n",
        "attn = F.dropout(attn, p=dropout_rate, training=True)  # Apply dropout\n",
        "x_standard = (attn @ v_single)  # Apply attention to value and reshape\n",
        "\n",
        "# Flash Attention Computation using scaled_dot_product_attention\n",
        "with torch.backends.cuda.sdp_kernel(\n",
        "    enable_flash=True, enable_math=False, enable_mem_efficient=False\n",
        "):\n",
        "    # Note: Assuming 'enable_flash' triggers FlashAttention internally in scaled_dot_product_attention\n",
        "    out_flash_sdp = F.scaled_dot_product_attention(q_single, k_single, v_single, dropout_p=dropout_rate)\n",
        "\n",
        "# # FlashAttention V2 Computation\n",
        "# out_flash_v2, _, _ = flash_attn_func(q_single, k_single, v_single, dropout_p=dropout_rate, return_attn_probs=True)\n",
        "\n",
        "# # FlashAttention QKV Packed Computation\n",
        "# out_flash_qkv_packed, _, _ = flash_attn_qkvpacked_func(qkv_single, dropout_p=dropout_rate, return_attn_probs=True)\n",
        "\n",
        "# Compare all the output4\n",
        "tolerance = 1e-1  # Tolerance level for floating-point comparisons\n",
        "if (torch.allclose(x_standard, out_flash_sdp, atol=tolerance)):\n",
        "    print('All attention implementations produce close enough results.')\n",
        "else:\n",
        "    print('There is a discrepancy between the attention implementations.')\n",
        "\n",
        "# if (torch.allclose(x_standard, out_flash_sdp, atol=tolerance) and\n",
        "#     torch.allclose(out_flash_sdp, out_flash_v2, atol=tolerance) and\n",
        "#     torch.allclose(out_flash_v2, out_flash_qkv_packed, atol=tolerance)):\n",
        "#     print('All attention implementations produce close enough results.')\n",
        "# else:\n",
        "#     print('There is a discrepancy between the attention implementations.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fEGM3eaetB0",
        "outputId": "7a67c0da-1be0-4127-f9b8-ebf4e0ac120f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is a discrepancy between the attention implementations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time Benchmark"
      ],
      "metadata": {
        "id": "NBMpYmofejiZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUZW57xDBN-T",
        "outputId": "a33f1a8b-97ff-4009-d6f0-53b82acbef53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard attention took 0.31433939933776855 seconds for 10 trials\n",
            "Flash attention took 0.10668373107910156 seconds for 10 trials\n",
            "FlashAttention V2 took 0.02718377113342285 seconds for 10 trials\n",
            "FlashAttention QKV Packed took 0.02114129066467285 seconds for 10 trials\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Creating query (q), key (k), and value (v) tensors\n",
        "# These tensors are initialized with random values and moved to the GPU for faster processing\n",
        "q = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "k = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "v = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "\n",
        "\n",
        "# Standard Attention Computation\n",
        "torch.cuda.synchronize()  # Synchronizes CPU and GPU to ensure accurate timing\n",
        "start = time.time()  # Start timer\n",
        "for i in range(num_trials):\n",
        "    attn = q @ k.transpose(-2, -1)  # Compute attention scores\n",
        "    attn = attn.softmax(dim=-1)  # Apply softmax to get probabilities\n",
        "    attn = F.dropout(attn, p=dropout_rate, training=True)  # Apply dropout\n",
        "    x = (attn @ v).transpose(1, 2)  # Apply attention to value and reshape\n",
        "torch.cuda.synchronize()  # Ensure all GPU tasks are finished\n",
        "end = time.time()  # End timer\n",
        "print('Standard attention took {} seconds for {} trials'.format(end - start, num_trials))\n",
        "\n",
        "# Flash Attention Computation\n",
        "with torch.backends.cuda.sdp_kernel(\n",
        "    enable_flash=True, enable_math=False, enable_mem_efficient=False\n",
        "):\n",
        "    torch.cuda.synchronize()  # Synchronizes CPU and GPU for accurate timing\n",
        "    start = time.time()  # Start timer\n",
        "    for i in range(num_trials):\n",
        "        out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_rate)  # Compute attention using FlashAttention\n",
        "    torch.cuda.synchronize()  # Ensure completion of all GPU tasks\n",
        "    end = time.time()  # End timer\n",
        "    print('Flash attention took {} seconds for {} trials'.format(end - start, num_trials))\n",
        "\n",
        "# FlashAttention V2 Computation (add this to your existing code)\n",
        "torch.cuda.synchronize()  # Synchronizes CPU and GPU for accurate timing\n",
        "start = time.time()  # Start timer\n",
        "for i in range(num_trials):\n",
        "    # Replace 'flash_attn_func' with the actual FlashAttention V2 function if it has a different name\n",
        "    out = flash_attn_func(q, k, v, dropout_p=dropout_rate)  # Compute attention using FlashAttention V2\n",
        "torch.cuda.synchronize()  # Ensure completion of all GPU tasks\n",
        "end = time.time()  # End timer\n",
        "print('FlashAttention V2 took {} seconds for {} trials'.format(end - start, num_trials))\n",
        "\n",
        "# FlashAttention QKV Packed Computation (add this to your existing code)\n",
        "torch.cuda.synchronize()  # Synchronizes CPU and GPU for accurate timing\n",
        "# Prepare the qkv tensor by stacking q, k, v along the third dimension\n",
        "qkv = torch.stack((q, k, v), dim=2)\n",
        "start = time.time()  # Start timer\n",
        "for i in range(num_trials):\n",
        "    # Call the flash_attn_qkvpacked_func with the stacked qkv tensor\n",
        "    out= flash_attn_qkvpacked_func(qkv, dropout_p=dropout_rate)\n",
        "torch.cuda.synchronize()  # Ensure completion of all GPU tasks\n",
        "end = time.time()  # End timer\n",
        "print('FlashAttention QKV Packed took {} seconds for {} trials'.format(end - start, num_trials))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVPZE-n6Y6vA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT Inference"
      ],
      "metadata": {
        "id": "rU933mvukjjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.eval()\n",
        "model.cuda()  # Move model to GPU\n",
        "\n",
        "# Initial input text\n",
        "input_text = \"two roads diverged \"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').cuda()\n",
        "\n",
        "# Function to generate 100 tokens autoregressively\n",
        "def generate_100_tokens(model, input_ids):\n",
        "    generated_text = input_ids\n",
        "    for _ in range(100):\n",
        "        outputs = model(generated_text)\n",
        "        next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
        "        generated_text = torch.cat([generated_text, next_token], dim=-1)\n",
        "    return generated_text\n",
        "\n",
        "# Measure inference time without FlashAttention\n",
        "torch.cuda.synchronize()\n",
        "start_time = time.time()\n",
        "generated_text_normal = generate_100_tokens(model, input_ids)\n",
        "torch.cuda.synchronize()\n",
        "normal_inference_time = time.time() - start_time\n",
        "\n",
        "print(f'Normal Inference Time: {normal_inference_time:.3f} seconds')\n",
        "\n",
        "# Measure inference time with FlashAttention\n",
        "with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
        "    torch.cuda.synchronize()\n",
        "    start_time = time.time()\n",
        "    generated_text_flash = generate_100_tokens(model, input_ids)\n",
        "    torch.cuda.synchronize()\n",
        "    flash_inference_time = time.time() - start_time\n",
        "print(f'FlashAttention Inference Time: {flash_inference_time:.3f} seconds')\n",
        "\n",
        "# Decode and print the generated text\n",
        "print(\"Generated Text without FlashAttention:\")\n",
        "print(tokenizer.decode(generated_text_normal[0]))\n",
        "\n",
        "print(\"\\nGenerated Text with FlashAttention:\")\n",
        "print(tokenizer.decode(generated_text_flash[0]))\n"
      ],
      "metadata": {
        "id": "XzschxlqlK37",
        "outputId": "4156621f-8669-482d-d607-79452757621c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normal Inference Time: 2.273 seconds\n",
            "FlashAttention Inference Time: 2.259 seconds\n",
            "Generated Text without FlashAttention:\n",
            "two roads diverged  from the road, and the road was closed.\n",
            "The road was closed for a few hours, and the road was reopened.\n",
            "The road was closed for a few hours, and the road was reopened.\n",
            "The road was closed for a few hours, and the road was reopened.\n",
            "The road was closed for a few hours, and the road was reopened.\n",
            "The road was closed for a few hours, and the road was reopened.\n",
            "The road was closed for a few hours\n",
            "\n",
            "Generated Text with FlashAttention:\n",
            "two roads diverged  from the road, and the road was closed.\n",
            "The road was closed for a few hours, and the road was reopened.\n",
            "The road was closed for a few hours, and the road was reopened.\n",
            "The road was closed for a few hours, and the road was reopened.\n",
            "The road was closed for a few hours, and the road was reopened.\n",
            "The road was closed for a few hours, and the road was reopened.\n",
            "The road was closed for a few hours\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_PnHAFZOlrRL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}