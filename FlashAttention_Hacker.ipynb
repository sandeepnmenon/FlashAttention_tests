{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeepnmenon/FlashAttention_tests/blob/master/FlashAttention_Hacker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhdoo7eCaKMK",
        "outputId": "dfa6a96d-3805-47d3-ab19-1009311fcdfc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.3.3.tar.gz (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.1.0+cu118)\n",
            "Collecting einops (from flash-attn)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (23.2)\n",
            "Collecting ninja (from flash-attn)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.3.3-cp310-cp310-linux_x86_64.whl size=57075008 sha256=bcb63b64213ab61590b340b77de84e448a442e19c100480895194df39ad7673d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/e6/fa/941802ec61d1afd320d27160ab1db98e6dba65381f84b76d4a\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: ninja, einops, flash-attn\n",
            "Successfully installed einops-0.7.0 flash-attn-2.3.3 ninja-1.11.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0KEIvTL_kaF",
        "outputId": "0d525575-3208-4b7e-8b51-110a1e81de73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from flash_attn import flash_attn_qkvpacked_func, flash_attn_func\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the basic parameters for the model\n",
        "batch_size = 32\n",
        "sequence_length = 2048\n",
        "dimensions = 64\n",
        "number_of_heads = 8\n",
        "\n",
        "# Define the dropout rate and number of trials for benchmarking\n",
        "dropout_rate = 0.0\n",
        "num_trials = 10\n"
      ],
      "metadata": {
        "id": "ATM_el16fd83"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FlashAttention might be flashy but it is not an Approximation"
      ],
      "metadata": {
        "id": "IiAJNYECemm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a single set of random q, k, v tensors\n",
        "q_single = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "k_single = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "v_single = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "qkv_single = torch.stack((q_single, k_single, v_single), dim=2)\n",
        "\n",
        "# Standard Attention Computation\n",
        "attn = q_single @ k_single.transpose(-2, -1)  # Compute attention scores\n",
        "attn = attn.softmax(dim=-1)  # Apply softmax to get probabilities\n",
        "attn = F.dropout(attn, p=dropout_rate, training=True)  # Apply dropout\n",
        "x_standard = (attn @ v_single)  # Apply attention to value and reshape\n",
        "\n",
        "# Flash Attention Computation using scaled_dot_product_attention\n",
        "with torch.backends.cuda.sdp_kernel(\n",
        "    enable_flash=True, enable_math=False, enable_mem_efficient=False\n",
        "):\n",
        "    # Note: Assuming 'enable_flash' triggers FlashAttention internally in scaled_dot_product_attention\n",
        "    out_flash_sdp = F.scaled_dot_product_attention(q_single, k_single, v_single, dropout_p=dropout_rate)\n",
        "\n",
        "# # FlashAttention V2 Computation\n",
        "# out_flash_v2, _, _ = flash_attn_func(q_single, k_single, v_single, dropout_p=dropout_rate, return_attn_probs=True)\n",
        "\n",
        "# # FlashAttention QKV Packed Computation\n",
        "# out_flash_qkv_packed, _, _ = flash_attn_qkvpacked_func(qkv_single, dropout_p=dropout_rate, return_attn_probs=True)\n",
        "\n",
        "# Compare all the output4\n",
        "tolerance = 1e-1  # Tolerance level for floating-point comparisons\n",
        "if (torch.allclose(x_standard, out_flash_sdp, atol=tolerance)):\n",
        "    print('All attention implementations produce close enough results.')\n",
        "else:\n",
        "    print('There is a discrepancy between the attention implementations.')\n",
        "\n",
        "# if (torch.allclose(x_standard, out_flash_sdp, atol=tolerance) and\n",
        "#     torch.allclose(out_flash_sdp, out_flash_v2, atol=tolerance) and\n",
        "#     torch.allclose(out_flash_v2, out_flash_qkv_packed, atol=tolerance)):\n",
        "#     print('All attention implementations produce close enough results.')\n",
        "# else:\n",
        "#     print('There is a discrepancy between the attention implementations.')"
      ],
      "metadata": {
        "id": "_fEGM3eaetB0",
        "outputId": "7a67c0da-1be0-4127-f9b8-ebf4e0ac120f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is a discrepancy between the attention implementations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time Benchmark"
      ],
      "metadata": {
        "id": "NBMpYmofejiZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUZW57xDBN-T",
        "outputId": "a33f1a8b-97ff-4009-d6f0-53b82acbef53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard attention took 0.31433939933776855 seconds for 10 trials\n",
            "Flash attention took 0.10668373107910156 seconds for 10 trials\n",
            "FlashAttention V2 took 0.02718377113342285 seconds for 10 trials\n",
            "FlashAttention QKV Packed took 0.02114129066467285 seconds for 10 trials\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Creating query (q), key (k), and value (v) tensors\n",
        "# These tensors are initialized with random values and moved to the GPU for faster processing\n",
        "q = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "k = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "v = torch.randn(batch_size, number_of_heads, sequence_length, dimensions, dtype=torch.float16).cuda()\n",
        "\n",
        "\n",
        "# Standard Attention Computation\n",
        "torch.cuda.synchronize()  # Synchronizes CPU and GPU to ensure accurate timing\n",
        "start = time.time()  # Start timer\n",
        "for i in range(num_trials):\n",
        "    attn = q @ k.transpose(-2, -1)  # Compute attention scores\n",
        "    attn = attn.softmax(dim=-1)  # Apply softmax to get probabilities\n",
        "    attn = F.dropout(attn, p=dropout_rate, training=True)  # Apply dropout\n",
        "    x = (attn @ v).transpose(1, 2)  # Apply attention to value and reshape\n",
        "torch.cuda.synchronize()  # Ensure all GPU tasks are finished\n",
        "end = time.time()  # End timer\n",
        "print('Standard attention took {} seconds for {} trials'.format(end - start, num_trials))\n",
        "\n",
        "# Flash Attention Computation\n",
        "with torch.backends.cuda.sdp_kernel(\n",
        "    enable_flash=True, enable_math=False, enable_mem_efficient=False\n",
        "):\n",
        "    torch.cuda.synchronize()  # Synchronizes CPU and GPU for accurate timing\n",
        "    start = time.time()  # Start timer\n",
        "    for i in range(num_trials):\n",
        "        out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_rate)  # Compute attention using FlashAttention\n",
        "    torch.cuda.synchronize()  # Ensure completion of all GPU tasks\n",
        "    end = time.time()  # End timer\n",
        "    print('Flash attention took {} seconds for {} trials'.format(end - start, num_trials))\n",
        "\n",
        "# FlashAttention V2 Computation (add this to your existing code)\n",
        "torch.cuda.synchronize()  # Synchronizes CPU and GPU for accurate timing\n",
        "start = time.time()  # Start timer\n",
        "for i in range(num_trials):\n",
        "    # Replace 'flash_attn_func' with the actual FlashAttention V2 function if it has a different name\n",
        "    out = flash_attn_func(q, k, v, dropout_p=dropout_rate)  # Compute attention using FlashAttention V2\n",
        "torch.cuda.synchronize()  # Ensure completion of all GPU tasks\n",
        "end = time.time()  # End timer\n",
        "print('FlashAttention V2 took {} seconds for {} trials'.format(end - start, num_trials))\n",
        "\n",
        "# FlashAttention QKV Packed Computation (add this to your existing code)\n",
        "torch.cuda.synchronize()  # Synchronizes CPU and GPU for accurate timing\n",
        "# Prepare the qkv tensor by stacking q, k, v along the third dimension\n",
        "qkv = torch.stack((q, k, v), dim=2)\n",
        "start = time.time()  # Start timer\n",
        "for i in range(num_trials):\n",
        "    # Call the flash_attn_qkvpacked_func with the stacked qkv tensor\n",
        "    out= flash_attn_qkvpacked_func(qkv, dropout_p=dropout_rate)\n",
        "torch.cuda.synchronize()  # Ensure completion of all GPU tasks\n",
        "end = time.time()  # End timer\n",
        "print('FlashAttention QKV Packed took {} seconds for {} trials'.format(end - start, num_trials))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVPZE-n6Y6vA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W-vLVG3qaJPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VKXL3-tZZLV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}